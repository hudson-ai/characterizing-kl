
@misc{baezBayesianCharacterizationRelative2014,
  title = {A {{Bayesian Characterization}} of {{Relative Entropy}}},
  author = {Baez, John C. and Fritz, Tobias},
  date = {2014-07-11},
  number = {arXiv:1402.3067},
  eprint = {1402.3067},
  eprinttype = {arxiv},
  primaryclass = {math-ph, physics:quant-ph},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1402.3067},
  urldate = {2022-05-26},
  abstract = {We give a new characterization of relative entropy, also known as the Kullback–Leibler divergence. We use a number of interesting categories related to probability theory. In particular, we consider a category FinStat where an object is a finite set equipped with a probability distribution, while a morphism is a measure-preserving function f : X → Y together with a stochastic right inverse s : Y → X. The function f can be thought of as a measurement process, while s provides a hypothesis about the state of the measured system given the result of a measurement. Given this data we can define the entropy of the probability distribution on X relative to the ‘prior’ given by pushing the probability distribution on Y forwards along s. We say that s is ‘optimal’ if these distributions agree. We show that any convex linear, lower semicontinuous functor from FinStat to the additive monoid [0, ∞] which vanishes when s is optimal must be a scalar multiple of this relative entropy. Our proof is independent of all earlier characterizations, but inspired by the work of Petz.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematical Physics,Mathematics - Probability,Primary 94A17; Secondary 62F15; 18B99,Quantum Physics},
  file = {/Users/hudson_home/Zotero/storage/ITB99N5D/Baez and Fritz - 2014 - A Bayesian Characterization of Relative Entropy.pdf}
}

@article{baezCharacterizationEntropyTerms2011,
  title = {A {{Characterization}} of {{Entropy}} in {{Terms}} of {{Information Loss}}},
  author = {Baez, John C. and Fritz, Tobias and Leinster, Tom},
  date = {2011-11-24},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {13},
  number = {11},
  pages = {1945--1957},
  issn = {1099-4300},
  doi = {10.3390/e13111945},
  url = {http://www.mdpi.com/1099-4300/13/11/1945},
  urldate = {2022-05-26},
  abstract = {There are numerous characterizations of Shannon entropy and Tsallis entropy as measures of information obeying certain properties. Using work by Faddeev and Furuichi, we derive a very simple characterization. Instead of focusing on the entropy of a probability measure on a finite set, this characterization focuses on the ‘information loss’, or change in entropy, associated with a measure-preserving function. We show that Shannon entropy gives the only concept of information loss that is functorial, convex-linear and continuous. This characterization naturally generalizes to Tsallis entropy as well.},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/E7F5PVJJ/Baez et al. - 2011 - A Characterization of Entropy in Terms of Informat.pdf}
}

@book{bungeDelawareSeminarFoundations1967,
  title = {Delaware {{Seminar}} in the {{Foundations}} of {{Physics}}},
  editor = {Bunge, Mario},
  date = {1967},
  series = {Studies in the {{Foundations Methodology}} and {{Philosophy}} of {{Science}}},
  volume = {1},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-86102-4},
  url = {http://link.springer.com/10.1007/978-3-642-86102-4},
  urldate = {2022-06-14},
  editorb = {Bunge, Mario and Bergmann, Peter G. and Flügge, Siegfried and Margenau, Henry and Medawar, Peter and Popper, Karl and Suppes, Patrick and Truesdell, Clifford A.},
  editorbtype = {redactor},
  isbn = {978-3-642-86104-8 978-3-642-86102-4},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/PW5GNRUT/Bunge - 1967 - Delaware Seminar in the Foundations of Physics.pdf}
}

@article{cohenRelativeEntropyMappings1993,
  title = {Relative Entropy under Mappings by Stochastic Matrices},
  author = {Cohen, Joel E. and Iwasa, Yoh and Rautu, Gh. and Beth Ruskai, Mary and Seneta, Eugene and Zbaganu, Gh.},
  date = {1993-01},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {179},
  pages = {211--235},
  issn = {00243795},
  doi = {10.1016/0024-3795(93)90331-H},
  url = {https://linkinghub.elsevier.com/retrieve/pii/002437959390331H},
  urldate = {2022-06-11},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/SVGAKPJN/Cohen et al. - 1993 - Relative entropy under mappings by stochastic matr.pdf}
}

@article{crooksBoltzmannGibbsStatisticsMaximum2007,
  title = {Beyond {{Boltzmann-Gibbs}} Statistics: {{Maximum}} Entropy Hyperensembles out-of-Equilibrium},
  shorttitle = {Beyond {{Boltzmann-Gibbs}} Statistics},
  author = {Crooks, Gavin E.},
  date = {2007-04-27},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {75},
  number = {4},
  eprint = {cond-mat/0603120},
  eprinttype = {arxiv},
  pages = {041119},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.041119},
  url = {http://arxiv.org/abs/cond-mat/0603120},
  urldate = {2019-12-12},
  abstract = {What is the best description that we can construct of a thermodynamic system that is not in equilibrium, given only one, or a few, extra parameters over and above those needed for a description of the same system at equilibrium? Here, we argue the most appropriate additional parameter is the non-equilibrium entropy of the system, and that we should not attempt to estimate the probability distribution of the system, but rather the metaprobability (or hyperensemble) that the system is described by a particular probability distribution. The result is an entropic distribution with two parameters, one a non-equilibrium temperature, and the other a measure of distance from equilibrium. This dispersion parameter smoothly interpolates between certainty of a canonical distribution at equilibrium and great uncertainty as to the probability distribution as we move away from equilibrium. We deduce that, in general, large, rare fluctuations become far more common as we move away from equilibrium.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics},
  file = {/Volumes/GoogleDrive-113334895637450796232/My Drive/Zotero/Dissipative Systems and Their Boundaries/Crooks - 2007 - Beyond Boltzmann-Gibbs statistics Maximum entropy.pdf}
}

@article{fullwoodInformationLossStochastic2021,
  title = {The {{Information Loss}} of a {{Stochastic Map}}},
  author = {Fullwood, James and Parzygnat, Arthur J.},
  date = {2021-08-08},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {23},
  number = {8},
  pages = {1021},
  issn = {1099-4300},
  doi = {10.3390/e23081021},
  url = {https://www.mdpi.com/1099-4300/23/8/1021},
  urldate = {2022-05-30},
  abstract = {We provide a stochastic extension of the Baez–Fritz–Leinster characterization of the Shannon information loss associated with a measure-preserving function. This recovers the conditional entropy and a closely related information-theoretic measure that we call conditional information loss. Although not functorial, these information measures are semi-functorial, a concept we introduce that is definable in any Markov category. We also introduce the notion of an entropic Bayes’ rule for information measures, and we provide a characterization of conditional entropy in terms of this rule.},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/QE9QKQ9H/Fullwood and Parzygnat - 2021 - The Information Loss of a Stochastic Map.pdf}
}

@article{gaveauGeneralFrameworkNonequilibrium1997,
  title = {A General Framework for Non-Equilibrium Phenomena: The Master Equation and Its Formal Consequences},
  shorttitle = {A General Framework for Non-Equilibrium Phenomena},
  author = {Gaveau, Bernard and Schulman, L.S.},
  date = {1997-06},
  journaltitle = {Physics Letters A},
  shortjournal = {Physics Letters A},
  volume = {229},
  number = {6},
  pages = {347--353},
  issn = {03759601},
  doi = {10.1016/S0375-9601(97)00185-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0375960197001850},
  urldate = {2022-06-14},
  abstract = {We consider non-equilibrium systems defined by a state space, and by a stochastic dynamics and its stationary state. The dynamics need nor satisfy detailed balance. In this abstract framework we do the following: (I) define and analyze “relative entropy”, (2) study dissipation in the relaxation to the stationary state, as well as the extra dissipation to maintain the system in its stationary state against some detailed balance dynamics, (3) extend the fluctuation-dissipation theorem and the Onsager relations, and (4) give a formula for the stationary state in terms of a summation over trees. 0 Elsevier Science B.V.},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/W3BHE3XG/Gaveau and Schulman - 1997 - A general framework for non-equilibrium phenomena.pdf}
}

@article{gorbanGeneralHtheoremEntropies2014,
  title = {General {{H-theorem}} and Entropies That Violate the Second Law},
  author = {Gorban, Alexander N.},
  date = {2014-04-29},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {16},
  number = {5},
  eprint = {1212.6767},
  eprinttype = {arxiv},
  primaryclass = {cond-mat},
  pages = {2408--2432},
  issn = {1099-4300},
  doi = {10.3390/e16052408},
  url = {http://arxiv.org/abs/1212.6767},
  urldate = {2022-06-14},
  abstract = {H-theorem states that the entropy production is nonnegative and, therefore, the entropy of a closed system should monotonically change in time. In information processing, the entropy production is positive for random transformation of signals (the information processing lemma). Originally, the H-theorem and the information processing lemma were proved for the classical Boltzmann-Gibbs-Shannon entropy and for the correspondent divergence (the relative entropy). Many new entropies and divergences have been proposed during last decades and for all of them the H-theorem is needed. This note proposes a simple and general criterion to check whether the H-theorem is valid for a convex divergence H and demonstrates that some of the popular divergences obey no H-theorem. We consider systems with n states Ai that obey first order kinetics (master equation). A convex function H is a Lyapunov function for all master equations with given equilibrium if and only if its conditional minima properly describe the equilibria of pair transitions Ai Aj. This theorem does not depend on the principle of detailed balance and is valid for general Markov kinetics. Elementary analysis of pair equilibria demonstrates that the popular Bregman divergences like Euclidean distance or Itakura-Saito distance in the space of distribution cannot be the universal Lyapunov functions for the first-order kinetics and can increase in Markov processes. Therefore, they violate the second law and the information processing lemma. In particular, for these measures of information (divergences) random manipulation with data may add information to data. The main results are extended to nonlinear generalized mass action law kinetic equations. In Appendix, a new family of the universal Lyapunov functions for the generalized mass action law kinetics is described.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics},
  file = {/Users/hudson_home/Zotero/storage/K3AE3VAI/Gorban - 2014 - General H-theorem and entropies that violate the s.pdf}
}

@article{horowitzMinimumEnergeticCost2017,
  title = {Minimum Energetic Cost to Maintain a Target Nonequilibrium State},
  author = {Horowitz, Jordan M. and Zhou, Kevin and England, Jeremy L.},
  date = {2017-04-04},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {95},
  number = {4},
  pages = {042102},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.95.042102},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.95.042102},
  urldate = {2021-12-26},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/TTKH3XZ8/Horowitz et al. - 2017 - Minimum energetic cost to maintain a target nonequ.pdf}
}

@article{kannappanCharacterizationDirectedDivergence1973,
  title = {On a Characterization of Directed Divergence},
  author = {Kannappan, P.L. and Rathie, P.N.},
  date = {1973-03},
  journaltitle = {Information and Control},
  shortjournal = {Information and Control},
  volume = {22},
  number = {2},
  pages = {163--171},
  issn = {00199958},
  doi = {10.1016/S0019-9958(73)90246-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995873902465},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/IF3T3JT2/Kannappan and Rathie - 1973 - On a characterization of directed divergence.pdf}
}

@article{kannappanMeasurableSolutionsFunctional1973,
  title = {Measurable Solutions of Functional Equations Related to Information Theory},
  author = {Kannappan, Pl. and Ng, C. T.},
  date = {1973},
  journaltitle = {Proceedings of the American Mathematical Society},
  shortjournal = {Proc. Amer. Math. Soc.},
  volume = {38},
  number = {2},
  pages = {303--310},
  issn = {0002-9939, 1088-6826},
  doi = {10.1090/S0002-9939-1973-0312110-1},
  url = {https://www.ams.org/proc/1973-038-02/S0002-9939-1973-0312110-1/},
  urldate = {2022-05-26},
  abstract = {Measurable solutions of functional equations connected with Shannon's measure of entropy, directed divergence or information gain and inaccuracy are found.},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/6F946ICJ/Kannappan and Ng - 1973 - Measurable solutions of functional equations relat.pdf}
}

@misc{leinsterEntropyDiversityAxiomatic2021,
  title = {Entropy and {{Diversity}}: {{The Axiomatic Approach}}},
  shorttitle = {Entropy and {{Diversity}}},
  author = {Leinster, Tom},
  date = {2021-06-22},
  number = {arXiv:2012.02113},
  eprint = {2012.02113},
  eprinttype = {arxiv},
  primaryclass = {cs, math, q-bio},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.02113},
  urldate = {2022-05-26},
  abstract = {This book brings new mathematical rigour to the ongoing vigorous debate on how to quantify biological diversity. The question "what is diversity?" has surprising mathematical depth, and breadth too: this book involves parts of mathematics ranging from information theory, functional equations and probability theory to category theory, geometric measure theory and number theory. It applies the power of the axiomatic method to a biological problem of pressing concern, but the new concepts and theorems are also motivated from a purely mathematical perspective. The main narrative thread requires no more than an undergraduate course in analysis. No familiarity with entropy or diversity is assumed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {92B99; 94A17; 39B99; 26E60; 18D50,Computer Science - Information Theory,Mathematics - Category Theory,Mathematics - Classical Analysis and ODEs,Quantitative Biology - Populations and Evolution,Quantitative Biology - Quantitative Methods},
  file = {/Users/hudson_home/Zotero/storage/P8TM777V/Leinster - 2021 - Entropy and Diversity The Axiomatic Approach.pdf}
}

@misc{leinsterShortCharacterizationRelative2017,
  title = {A Short Characterization of Relative Entropy},
  author = {Leinster, Tom},
  date = {2017-12-13},
  number = {arXiv:1712.04903},
  eprint = {1712.04903},
  eprinttype = {arxiv},
  primaryclass = {math-ph, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1712.04903},
  urldate = {2022-05-26},
  abstract = {We prove characterization theorems for relative entropy (also known as Kullback–Leibler divergence), q-logarithmic entropy (also known as Tsallis entropy), and q-logarithmic relative entropy. All three have been characterized axiomatically before, but we show that earlier proofs can be simplified considerably, at the same time relaxing some of the hypotheses.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematical Physics,Mathematics - Statistics Theory},
  file = {/Users/hudson_home/Zotero/storage/XMXNCCN5/Leinster - 2017 - A short characterization of relative entropy.pdf}
}

@article{petzCharacterizationRelativeEntropy1992,
  title = {Characterization of the Relative Entropy of States of Matrix Algebras},
  author = {Petz, D.},
  date = {1992},
  journaltitle = {Acta Mathematica Hungarica},
  shortjournal = {Acta Math Hung},
  volume = {59},
  number = {3-4},
  pages = {449--455},
  issn = {0236-5294, 1588-2632},
  doi = {10.1007/BF00050907},
  url = {http://link.springer.com/10.1007/BF00050907},
  urldate = {2022-06-01},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/XV3AIAL6/Petz - 1992 - Characterization of the relative entropy of states.pdf}
}

@article{qianRelativeEntropyFree2001,
  title = {Relative Entropy: {{Free}} Energy Associated with Equilibrium Fluctuations and Nonequilibrium Deviations},
  shorttitle = {Relative Entropy},
  author = {Qian, Hong},
  date = {2001-03-26},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {63},
  number = {4},
  pages = {042103},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.63.042103},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.63.042103},
  urldate = {2022-06-14},
  langid = {english},
  file = {/Users/hudson_home/Zotero/storage/VNPI8WFT/Qian - 2001 - Relative entropy Free energy associated with equi.pdf}
}

@book{shawDrippingFaucetModel1984,
  title = {The Dripping Faucet as a Model Chaotic System},
  author = {Shaw, Robert},
  date = {1984},
  series = {The {{Science}} Frontier Express Series},
  publisher = {{Aerial Press}},
  location = {{Santa Cruz, CA}},
  isbn = {978-0-942344-05-9},
  pagetotal = {111},
  keywords = {Chaotic behavior in systems,Entropy (Information theory),Information theory,Mathematical models},
  file = {/Users/hudson_home/Zotero/storage/WAMSUG52/ShawRobert-DrippingFaucetAsAModelChaoticSystem1984.pdf}
}


